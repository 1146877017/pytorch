# This file serves an analysis script for the results generated by `zero.py`.

import os
import argparse
import csv
import json
import numpy as np


def load_exp_info(dirname):
    r"""
    Loads the experiment info from the file `exp_info.json`.

    Arguments:
        dirname (str): dirname containing the .json file.'
    """
    exp_info_filename = os.path.join(dirname, "exp_info.json")
    with open(exp_info_filename, 'r') as f:
        return json.load(f)


def main():
    r"""
    Analyzes the ZeRO experiment results.

    An example run command for analyzing the results for the greedy and the
    greedy-sorted partitioning algorithms found in "greedy/" and
    "greedy_sorted/" respectively is as follows:
        python analyze.py greedy greedy_sorted

    The argument parser expects the dirnames containing the experiment
    results generated by `zero.py`. Each such directory should contain a .json
    file c
    """
    # parse the dirnames containing the experiment output
    parser = argparse.ArgumentParser(description="PyTorch ZeRO experiment"
                                                 " analysis")
    parser.add_argument("dirnames", nargs='+')
    args = parser.parse_args()

    # analyze the experiment output
    for dirname in args.dirnames:
        print("-----------------------------------")
        print(dirname)
        print("-----------------------------------")

        # load the experiment info
        experiment_info = load_exp_info(dirname)
        world_sizes = experiment_info["world_sizes"]
        model_names = experiment_info["model_names"]

        # print statistics for each (world_size, model) configuration
        for world_size in world_sizes:
            for model_name in model_names:
                proc_num_params_list = list()
                optimizer_step_time_list = list()
                optimizer_step_time_std_list = list()

                # extract the info for the given rank
                for rank in range(world_size):
                    csv_filename = os.path.join(dirname, "%s_%d_%d.csv" %
                                                (model_name, world_size, rank))
                    with open(csv_filename, 'r', newline='') as csvfile:
                        reader = csv.reader(csvfile)
                        rows = list(reader)
                        assert len(rows) == 1, \
                            ".csv file should only contain one row"
                        row = rows[0]

                        # row: (model_name, rank, proc_num_params,
                        # optimizer_step_time, optimizer_step_time_std)
                        proc_num_params_list.append(int(row[2]))
                        optimizer_step_time_list.append(float(row[3]))
                        optimizer_step_time_std_list.append(float(row[4]))

                # compute the statistics
                max_num_params = max(proc_num_params_list)
                mean_num_params = np.mean(proc_num_params_list)
                max_optimizer_step_time = max(optimizer_step_time_list)
                argmax_idx = np.argmax(optimizer_step_time_list)
                max_optimizer_step_time_std = \
                    optimizer_step_time_std_list[argmax_idx]

                # print the statistics
                print("world_size=%d model_name=%s" % (world_size, model_name))
                print("max params     = %d" % max_num_params)
                print("mean params    = %.3f" % mean_num_params)
                print("diff           = %.3f" %
                      (max_num_params - mean_num_params))
                print("max time (std) = %.3f (%.5f)" %
                      (max_optimizer_step_time, max_optimizer_step_time_std))
                print()


if __name__ == "__main__":
    main()
